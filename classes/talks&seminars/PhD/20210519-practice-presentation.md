 Hello everyone and welcome to this new tutorial.

[What is this tutorial about?](https://www.youtube.com/watch?v=qu2Ep_qAFOQ)

I wanted to be meta in my introduction to this presentation citing youtube culture, but also because this presentation can be seen as both a presentation or a tutorial, because I will guide you in the making of nasty, gritty and noisy sounds generated by intercepting data that youtube sends to its own servers when viewers use their platform.

To do this I will follow the following structure:

1. recap from the last session expanding with the topic of primitive accumulation

2. [Showing how labor is extracted in the form of hovering information](#Showing-how-labor-is-extracted-in-the-form-of-hovering-information)

3. [Showing how to build a tool to intercept such information and turn it into musical information](#Showing-how-to-build-a-tool-to-intercept-such-information-and-turn-it-into-musical-information)

4. [How can we describe it as method?](#How-can-we-describe-it-as-method?)

5. final performance





## recap and some primitive accumulation

I will start by citing myself

> What I'm tapping into here, in my view, is the tip of a very complex iceberg in which users and creators are bound by an algorithm, the recommender algorithm that extracts free labour in form of manufactured data from users, and forces creators in constantly creating new content in order to reach an audience and turn an hobby into "profitable" labour.

If you remember in my last presentation I brought to you my interest in dissecting the how youtube uses AI to predict which videos a user might want to watch. Various scholars have talked about it in different ways, Seaver describes them as entrapment mechanisms, due to the fact that the algorithm by constantly providing **"fresh"** content locks the users in constant FOMO mode.
I want also to **"refresh"** you about the concept of **"freshness"** developed to make the recommender algorithm more profitable. In the case of the youtube recommender system **"freshness"** describes how new the video is so that the algorithm can narrow its search in the vast gulf of videos. Additional to that the recommender system needs to know other things about the user in order to look for the right video to show. Keyword here is **"engagement"** another of those silicon valley buzzwords for metric. **"Engagement"** measures how much time the user has spent on a video, YouTube jargon is called a ***viewing session***, whether it has stopped it, or closed it before it came to the end, the latter behaviour is described as ***session end***. Those behaviours are recorded and they influence how the recommender system is going to pick new videos for the feed. 

> ‘Session Starts’ is essentially how many people start their YouTube viewership session with one of your videos. ‘Session Duration’ is how long your content keeps people on the platform as they are watching your video, as well as after they’ve watched your video. ‘Session Ends’ relates to how often someone terminates a YouTube session while or after watching one of your videos. This is a negative metric to the algorithm. (Gielen and Rosen, 2016)

This way of assessing metrics is deemed as a way to create exclusion as Sophie Bishop argues

> An algorithm valuing videos inspiring extended viewing sessions on YouTube arguably skews towards punishing diverse, riskier and original content that may accrue session ends more frequently.

additional to that Paula Ricaurte also sees exclusion as embedded into social media platforms: refusing to generate data means exclusion

Creating exclusion and boundaries is a very lucrative business model, as Marx already described as Primitive accumulation that was successively dubbed as accumulation by dispossession by Adam Harvey.

Let's then have a look at it, [Shall we?](https://youtu.be/hBtaU2ESQAU?t=48) (Cit. Joshua Weismann)

To understand what primitive accumulation is we need to look into the Labor theory of property formulated by John Locke. in his two "treatises on government" Locke, inspired by land grabbing happening in north America, produced a theory that states that labor, therefore cultivation, can turn a natural resource into private property. Therefore, briefly explained, if European come to north America and they cultivate the land, that land becomes property of the Europeans. Since the indigenous people did not cultivate the land they have no right over it. Furthermore in the 18th century the figure of Adam Smith creates another myth around privatisation creating the dichotomy of the lazy and laborious worker, and the natural right of the latter to claim right over land and the right over the means of production, constructed through the wealth he accumulated thanks to his "natural" tendency to be rich.
It is within such frame that indigenous people slowly deprived of the land they inhabited for centuries, need to sell their labor in order to access the means of substinence aka food. And this process of separating a population from their means of production that forms a prehistory of capital, the so called primitive accumulation. It is to be noted that what described here it is a form of primitive accumulation pertinent to territory and culture of a specific epoch. Marx himself acknowledges that forms of primitive accumulation might vary between territories and cultures in time, but it nevertheless creates inequity and exploitation. Another thing to keep in mind here are words of John Locke "whatever I work upon I have the right to own it" and the dichotomy of the lazy and laborious formulated by Adam Smith.

This brings us to the 21st century. How does what described earlier reflect in the current epoch? How can we connect Primitive accumulation with the current data driven economies of tech corporations?

To answer to this question I will try to explore and discuss with you the ways in which data has become the so called oil of the 21st century, and how data driven technologies re-enact this form of primitive accumulation.

The current landscape of critical studies of data driven technology draws a connection between the ways in which users are manipulated into using social media platform the production of data points that are accumulated within the servers of tech companies. Some academics, like Couldry, Thatcher and Paula Ricaurte, refer to it as *data colonialism*, specially in reference to the extractive practice that such companies deploy in order to gather personal information from the users of their platforms. Here I need to make a note as I don't really agree with the term data extraction, because it implies that data exist in a raw natural form, but data if anything is neither natural nor raw. This because data is always created at the interaction between user and platform; therefore data, in my opinion, rather than extracted is manufactured.
To explain this better I will cite a couple of sources starting from Sandro Mezzadra, who building upon concepts of Kalindi vora states:

> As Vora has recently shown, this opening of the human body as a site for annexation, harvest, and production has strong resonances and continuities with land plundering and natural resource dispossession under European territorial colonialism

On a same note Jim Thatcher, David O'Sullivan and Dillon Mahmoudi say

> Big data is a colonial policy, but it is one in which, rather than opening the idealized markets of digital frontierism’s problematic imaginary, we have become subject to them. Sensors quantify, alienate, and extract conceptions of self, reducing life as seen by capital to what can be recorded and exchanged as digital data. Linked together in aggregate, the sum total of data produced by an individual marks them into an abstracted bucket, a digital commodity that may be continually bought and sold in order to call forth an orderly, predictable stream of consumption

In both of their texts the authors refer on how forms of primitive accumulation play a role in the re-enactment of colonial practices in order to accumulate data. The authors are aware of how Marx described primitive accumulation, and as stated earlier, primitive accumulation happens when a population is separated by their means of production. But what are the means of production in the case of social media users, and how are they separated from the users?

Data manufacture as shown before is so deeply hidden and alienated from the user that he barely knows that he participates in the manufacture of it, de facto detaching him from what he produces. to the point that the user does not even know what he produces. therefore if I was to use the word extraction, I would not say that data is extracted, but rather it is free labour, in the form of data manufacture, that is extracted by social media users.
To relate back to Lockian "labour theory of property" it is easy to imagine how social media company see the users behaviors as land or territory rich of raw data to be claimed proprietary by harvesting it. And I know this may sound confusing and contradictory, is it manufacture, extraction or harvest? In my opinion none and all of this words are correct, what i'm trying to present here is how this old way of thinking that nature is there to be turned into profit, whether it is "inferior" human bodies, materials or online behaviours, is still very much alive within the ways in which data-driven technologies operate nowadays. To make it short the modes by which this "extraction/harvest/manufacture" happens have changed but the motives are the same. From my perspective data is rather manufactured, but the narratives that circulate within the data-driven technology are of another idea, that is more aligned with the lockian labour theory of property, in which the world is just a big amount of data to be analysed and the process of analysis similar to the harvesting of land in the colonial times turns the data into a private property to be sold in the advertising market.



## Showing how labor is extracted in the form of hovering information

## Showing how to build a tool to intercept such information and turn it into musical information

## How can we describe it as method?

So in this part I would like to reflect on the method I employed, and to do so  I will reflect on something I wrote today for myself.

I will start with a bold statement: the performance is the method.
What do I mean by that?
In this first part of my research I wanted to understand how data extraction happens, and I wanted to be able to make this alienated process visible. And in order to do so I really needed to get into the guts of social media platform data collection algorithm. Because if I wouldn't make it visible for myself I couldn't make visible for anyone: obviously.
And as shown before the way into the guts of the data extraction processes was quite convoluted but nevertheless I was able to collect a piece of evidence a single moment that shows how the hovering over thumbnails is collected as it was important information to be stored. De facto dispossessing the user of the milliseconds he pointed the mouse over a moving thumbnail.
This crucial moment is the one that I chose to transform into audible mess, but the audible mess was decided *a priori* I knew before I started researching the algorithm that I wanted to sonify it. Therefore the sonification of the hovering was existing already prior of the discovery, the sonification in itself made me search for the hovering. Because without the hovering there wouldn't be neither sonification, and therefore no performance.
Said like this it seems as obvious as it gets, but it is in its naïveté that the complexity of the performance as the method shines; because in order to get to the performance I needed to employ several methodologies borrowed in my case  from ethnography and computer science.

From Kitchin I borrowed this methods:

* Examining pseudo-code/source code
* Reflexively producing code
* Reverse engineering
* Examining how algorithms do work in the world => to do this I took classes at the youtube academy, and collected evidence of algorithm imaginaries from youtube users

I did all of them but non of them thoroughly enough to be able to claim that that one is my method.
But nevertheless it was through the combination of this many methods that I got to this point, and this because in order to get to a sonification I needed to examine the code and reverse engineer it, as well as try to recreate it to better understand how it works and operate. But nevertheless I never fully did those thing I "sandboxed" my methods to gain the knowledge needed to proceed toward the sonification.



With that said we should hear some noises: [setting a timer for 10 minutes](https://www.youtube.com/watch?v=Fkg1ziEK_-Y)





# Bibliography


# Bibliography
